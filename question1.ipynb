{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMAdi9qgC-B9"
      },
      "source": [
        "To copy this template: File -> Save a Copy in Drive\n",
        "\n",
        "***DISCLAIMER**: In case of any discrepancy in the assignment instruction, please refer to the `PDF` document.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 1 - Sentiment Analysis using recurrent models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPfhSubI9X88",
        "outputId": "709f9921-9088-4e7d-dc9c-bf21836795f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-13 20:13:25.951066: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-11-13 20:13:28.542038: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-13 20:13:28.542072: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-13 20:13:28.542113: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-13 20:13:28.624291: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-13 20:13:32.011696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9fCP-n4Nmf60",
        "outputId": "761235ce-d6a6-4614-b6e9-b982cd4ff4c1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production. <br /><br />The...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\", usecols=[\"review\", \"sentiment\"], encoding='latin-1')\n",
        "## 1 - positive, 0 - negative\n",
        "df.sentiment = (df.sentiment == \"positive\").astype(\"int\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J59HaMAB0Ok",
        "outputId": "01965cec-5eb2-4612-aa19-9e7efaae16bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35000 7499 7499\n",
            "35000 35000 7499\n"
          ]
        }
      ],
      "source": [
        "val_size = int(df.shape[0] * 0.15)\n",
        "test_size = int(df.shape[0] * 0.15)\n",
        "\n",
        "\n",
        "def train_val_test_split(df=None, train_percent=0.7, test_percent=0.15, val_percent=0.15):\n",
        "  df = df.sample(frac=1)\n",
        "  train_df = df[: int(len(df)*train_percent)]\n",
        "  test_df = df[int(len(df)*train_percent)+1 : int(len(df)*(train_percent+test_percent))]\n",
        "  val_df = df[int(len(df)*(train_percent + test_percent))+1 : ]\n",
        "  return train_df, test_df, val_df\n",
        "\n",
        "train_df, test_df, val_df = train_val_test_split(df, 0.7, 0.15, 0.15)\n",
        "train_labels, train_texts = train_df.values[:,1], train_df.values[:,0]\n",
        "val_labels, val_texts = val_df.values[:,1], val_df.values[:,0]\n",
        "test_labels, test_texts = test_df.values[:,1], test_df.values[:,0]\n",
        "print(len(train_df), len(test_df), len(val_df))\n",
        "print(len(train_texts), len(train_labels), len(val_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-sh7CvzLQbx",
        "outputId": "a42cf786-79f1-47d6-9d95-ff20a69990c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['You have to be awfully patient to sit through a film with one-liners so flat and unfunny that you wonder what all the fuss was about when WHISTLING IN THE DARK opened to such an enthusiastic greeting from audiences in the 1940s.<br /><br />On top of some weak one-liners and ordinary sight gags, the plot is as far-fetched as the tales The Fox (Red Skelton) tells his radio audience. You have to wonder why anyone would think he could come up with a real-life solution on how to commit the perfect crime and get away with it. But then, that\\'s how unrealistic the comedy is.<br /><br />But--if you\\'re a true Red Skelton fan and enjoy a look back at how comedies were made in the \\'40s--you can at least enjoy the amiable cast supporting him. Ann Rutherford and Virginia Grey do nicely as his love interest and Conrad Veidt, as always, makes an interesting villain. One of his more amusing moments is his reaction to Skelton explaining the mysteries of wearing turbans. \"I never knew that,\" he muses, impressed by a minor point that is cleverly introduced.<br /><br />All in all, typical nonsense that requires you to accept the lack of credibility and just accept the gags as they are. Not always easy for a discriminating viewer as many of them simply fall flat, the way many comedies of this era do because the novelty of the sight gags and one-liners has simply worn off.',\n",
              "       'I first read the book, when I was a young teenager, then saw the film late one night. About a year ago I checked it out on IMDb and discovered no copies available. I then hit the web and found a site that offers War Films, soooo glad that I did, ordered a copy and sat back and was able to confirm why I wanted to see it again.<br /><br />In my opinion to really enjoy the film I suggest you read get a copy of the book and then watch the film. The book is no longer in print but I did track a copy down via E-bay, the Author Alan White was a commando/paratrooper during the 2nd world war taking part in disparate clandestine operations and this was his first book. It is written by someone who knows and this fact I believe gives the book and film authenticity. I have not given the film a ten only because of the nature of the ending of the film, not as good as the book. There are a couple of plot lines that differ from the book also, which is strange as the book is not about the large scale nature of war but about the individual in war. The film illustrates this exceptionally well. I have the copy of the book to let my son read and then the film to let him watch, in that order.<br /><br />If you can track it down the book and the film then it is definitely worth it and I only wish that it was more readily available for more to read and see, one of my all best war films, ever!',\n",
              "       '***Possible spoilers***<br /><br />I\\'ve read up on Dahmer a little and saw the new Dahmer film (with the same name) at an earlier time. This movie here concentrates rather much on the victims and killings, too little on Dahmer himself. The film called \"Dahmer\" had the opposite problem, it was too little about his crimes and too much about himself.<br /><br />I did not find the acting to my satisfaction, it had a certain amateur feel too it, especially the probation officer. It also seemed as if the Dahmer acting got worse every time he played against the probation officer actor. But I might be wrong about that.<br /><br />What annoyed me a bit was that some of the scenes were quite disturbing but that the filmmakers seemed to try and show \"the real deal\" about what he did anyway. That is ok - but what I then don\\'t understand is why the guy who ran away from his flat while Dahmer was out getting beer, was not depicted being naked, since that is also how it happened. It\\'s not a big deal, but it just eats away further on the movies quality that such details are left out. What wasn\\'t shown either or not even really hinted was Dahmers sexual obsession with the dead. Again, I don\\'t mind they didn\\'t SHOW it, but at least they could have mentioned it or built it in to the movie somehow.<br /><br />Conclusion: I think the really good Dahmer film is still to be made, a movie that incorporates not only Dahmers crimes but also who he was, and why he did what he did. I think that 1.5 or 2 hours are just not enough to grasp the complexity of it all. This movie was just a cutout (excuse the pun) of Dahmers life and personality and does not give you any \\'close to good\\' insight into his life or personality.<br /><br />4/10',\n",
              "       \"Before there was Crash, there was this interesting film called Grand Canyon. Released about 14 years sooner than the former film, Grand Canyon was a movie about two people from different backgrounds who come together as friends over a lifetime. To me Crash was still a slightly better film, but Grand Canyon was no slouch either.<br /><br />Taking place in Los Angeles, an upper-class lawyer named Mack (Kevin Kline) takes a shortcut through the seedier side of town only to have his car break down at the worst time. He calls for a tow truck, and has to wait for awhile, only to soon be threatened by a group of dangerous people who want his car. Soon the tow truck driver arrives at the perfect moment, and out steps Simon (Danny Glover) to take the truck away. Both men are threatened, but Simon manages to get himself, Mack, and the car out of dire straits. It is from here on out that a friendship develops between the two men over a lifetime with Mack helping out Simon just as Simon had helped him out of a dangerous situation earlier. You see Simon's sister Deborah (Tina Lifford) is living in a dangerous neighborhood with her two children, and fears for her oldest son who seems to be roaming the streets at night with some bad people. Mack offers them a better place to live as well as hooking Simon up with his secretary's friend Jane (Alfre Woodard).<br /><br />This is the main plot of the film, but there are other smaller plots involving the same secretary mentioned above (Mary Louise Parker) as well as Mack's wife, (Mary McDonnel) who discovers an abandoned baby not long after their son Roberto (Jeremy Sisto in his first movie role) has gone to camp for the summer, and will likely be moving on with his own life soon. The details of all these plots are brought together into one complex movie which uses a police helicopter as a metaphor for life and as a bridge to entwine all the different scenes. This simple plot device works very well and helps greatly with the flow of the story.<br /><br />The director Lawrence Kasdan, whose biggest movie to this date was The Big Chill, has created a splendid movie here. The cast is excellent, and most of the ideas are well thought out, but alas it falls short of greatness because some points, that would've made the film even stronger, are glossed over. The story involving the secretary is one, and the second involving Simon's nephew is the other. These scenes should've been more apart of the entire story, and then maybe Lawrence Kasdan's views of life between the upper and lower classes would've been more on a superior level instead of just very good. Still Grand Canyon exceeded expectations, and yes you will get to see a view of the canyon that this movie was named after. There is also a small role for Steve Martin as Davis, a producer of violent films, who offers his own views on life, and has a small part to play in this movie's ideas.\",\n",
              "       \"Coming from Oz I probably shouldn't say it but I find a lot of the local movies lacking that cohesive flow with a weak storyline. This comedy lacks in nothing. Great story, no overacting, no melodrama, just brilliant comedy as we know Oz can do it. Do yourself a favour and laugh till you drop.\"],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_texts[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X29iNgqoLXSb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "def process_tokens(text):\n",
        "    \"\"\"\n",
        "    function to process tokens, replace any unwanted chars\n",
        "    \"\"\"\n",
        "    preprocessed_text = text.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\":\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\"(\", \"\")\n",
        "    preprocessed_text = ''.join([i for i in preprocessed_text if not preprocessed_text.isdigit()])\n",
        "    return preprocessed_text\n",
        "\n",
        "def preprocessing(data):\n",
        "    \"\"\"\n",
        "    preprocessing data to list of tokens\n",
        "    \"\"\"\n",
        "    nlp = English()\n",
        "    tokenizer = Tokenizer(nlp.vocab)\n",
        "    preprocessed_data = []\n",
        "    for sentence in data:\n",
        "        sentence = process_tokens(sentence)\n",
        "        tokens = tokenizer(sentence)\n",
        "        tlist = []\n",
        "        for token in tokens:\n",
        "            tlist.append(str(token))\n",
        "        preprocessed_data.append(tlist)\n",
        "    return preprocessed_data\n",
        "\n",
        "train_data = preprocessing(train_texts)\n",
        "val_data = preprocessing(val_texts)\n",
        "test_data = preprocessing(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzulNnqSLaeQ",
        "outputId": "6e0c94af-74b2-4a6f-e0d3-3f63257e8da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['you', 'have', 'to', 'be', 'awfully', 'patient', 'to', 'sit', 'through', 'a', 'film', 'with', 'oneliners', 'so', 'flat', 'and', 'unfunny', 'that', 'you', 'wonder', 'what', 'all', 'the', 'fuss', 'was', 'about', 'when', 'whistling', 'in', 'the', 'dark', 'opened', 'to', 'such', 'an', 'enthusiastic', 'greeting', 'from', 'audiences', 'in', 'the', '1940s<br', '/><br', '/>on', 'top', 'of', 'some', 'weak', 'oneliners', 'and', 'ordinary', 'sight', 'gags', 'the', 'plot', 'is', 'as', 'farfetched', 'as', 'the', 'tales', 'the', 'fox', 'red', 'skelton', 'tells', 'his', 'radio', 'audience', 'you', 'have', 'to', 'wonder', 'why', 'anyone', 'would', 'think', 'he', 'could', 'come', 'up', 'with', 'a', 'reallife', 'solution', 'on', 'how', 'to', 'commit', 'the', 'perfect', 'crime', 'and', 'get', 'away', 'with', 'it', 'but', 'then', \"that's\", 'how', 'unrealistic', 'the', 'comedy', 'is<br', '/><br', '/>butif', \"you're\", 'a', 'true', 'red', 'skelton', 'fan', 'and', 'enjoy', 'a', 'look', 'back', 'at', 'how', 'comedies', 'were', 'made', 'in', 'the', \"'40syou\", 'can', 'at', 'least', 'enjoy', 'the', 'amiable', 'cast', 'supporting', 'him', 'ann', 'rutherford', 'and', 'virginia', 'grey', 'do', 'nicely', 'as', 'his', 'love', 'interest', 'and', 'conrad', 'veidt', 'as', 'always', 'makes', 'an', 'interesting', 'villain', 'one', 'of', 'his', 'more', 'amusing', 'moments', 'is', 'his', 'reaction', 'to', 'skelton', 'explaining', 'the', 'mysteries', 'of', 'wearing', 'turbans', '\"i', 'never', 'knew', 'that\"', 'he', 'muses', 'impressed', 'by', 'a', 'minor', 'point', 'that', 'is', 'cleverly', 'introduced<br', '/><br', '/>all', 'in', 'all', 'typical', 'nonsense', 'that', 'requires', 'you', 'to', 'accept', 'the', 'lack', 'of', 'credibility', 'and', 'just', 'accept', 'the', 'gags', 'as', 'they', 'are', 'not', 'always', 'easy', 'for', 'a', 'discriminating', 'viewer', 'as', 'many', 'of', 'them', 'simply', 'fall', 'flat', 'the', 'way', 'many', 'comedies', 'of', 'this', 'era', 'do', 'because', 'the', 'novelty', 'of', 'the', 'sight', 'gags', 'and', 'oneliners', 'has', 'simply', 'worn', 'off']\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNvU5O6cLbBg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "## Creating a vectorizer to vectorize text and create matrix of features\n",
        "## Bag of words technique\n",
        "class Vectorizer():\n",
        "    def __init__(self, max_features):\n",
        "        self.max_features = max_features\n",
        "        self.vocab_list = None\n",
        "        self.token_to_index = None\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        word_dict = {}\n",
        "        for sentence in dataset:\n",
        "            for token in sentence:\n",
        "                if token not in word_dict:\n",
        "                    word_dict[token] = 1\n",
        "                else:\n",
        "                    word_dict[token] += 1\n",
        "        word_dict = dict(sorted(word_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "        end_to_slice = min(len(word_dict), self.max_features)\n",
        "        word_dict = dict(itertools.islice(word_dict.items(), end_to_slice))\n",
        "        self.vocab_list = list(word_dict.keys())\n",
        "        self.token_to_index = {}\n",
        "        counter = 0\n",
        "        for token in self.vocab_list:\n",
        "            self.token_to_index[token] = counter\n",
        "            counter += 1\n",
        "\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
        "        for i, sentence in enumerate(dataset):\n",
        "            for token in sentence:\n",
        "                if token in self.token_to_index:\n",
        "                    data_matrix[i, self.token_to_index[token]] += 1\n",
        "        return data_matrix\n",
        "\n",
        "## max features - top k words to consider only\n",
        "max_features = 2000\n",
        "\n",
        "vectorizer = Vectorizer(max_features=max_features)\n",
        "vectorizer.fit(train_data)\n",
        "\n",
        "## Checking if the len of vocab = k\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "vocab = vectorizer.vocab_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgi_2rtWTZce",
        "outputId": "4592e4eb-b215-4936-f884-4ce806a20f29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[16.,  8.,  6., ...,  0.,  0.,  0.],\n",
              "       [25., 12.,  9., ...,  0.,  0.,  0.],\n",
              "       [18.,  7.,  6., ...,  0.,  0.,  0.],\n",
              "       [28., 13., 20., ...,  0.,  0.,  0.],\n",
              "       [ 1.,  1.,  3., ...,  0.,  0.,  0.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## each sequence of token is a vector of\n",
        "## token indices (with the count of those words)\n",
        "X_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSbjxMwtTbVY"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.astype('int')\n",
        "y_val = y_val.astype('int')\n",
        "y_test = y_test.astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0q1VJx-Tdwo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train, 2)\n",
        "y_test = to_categorical(y_test, 2)\n",
        "y_val = to_categorical(y_val, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XofP71pJTfq_",
        "outputId": "e23bec7a-c635-46e9-86ef-9954f631e62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train.shape: (35000, 1, 2000), y_train.shape: (35000, 2)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape(-1, 1, X_train.shape[1])\n",
        "X_val = X_val.reshape(-1, 1, X_val.shape[1])\n",
        "X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
        "\n",
        "y_train = y_train.reshape(-1, 2)\n",
        "y_val = y_val.reshape(-1, 2)\n",
        "y_test = y_test.reshape(-1, 2)\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrwkWPe09nMq",
        "outputId": "35ccb20d-8898-4c51-ab78-7ce13d40260a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-13 20:14:44.216329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 44333 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 256)               577792    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 578306 (2.21 MB)\n",
            "Trainable params: 578306 (2.21 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-13 20:14:48.871673: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x151d34909b90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-11-13 20:14:48.871707: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 8000, Compute Capability 7.5\n",
            "2023-11-13 20:14:48.877663: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-11-13 20:14:49.223943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
            "2023-11-13 20:14:49.333203: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "137/137 [==============================] - 4s 9ms/step - loss: 0.5389 - accuracy: 0.8077 - val_loss: 0.3048 - val_accuracy: 0.8672\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.2667 - accuracy: 0.8885 - val_loss: 0.2911 - val_accuracy: 0.8733\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.2276 - accuracy: 0.9064 - val_loss: 0.2996 - val_accuracy: 0.8729\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1798 - accuracy: 0.9287 - val_loss: 0.4054 - val_accuracy: 0.8476\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1237 - accuracy: 0.9553 - val_loss: 0.3688 - val_accuracy: 0.8684\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0661 - accuracy: 0.9809 - val_loss: 0.4072 - val_accuracy: 0.8708\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0380 - accuracy: 0.9908 - val_loss: 0.4466 - val_accuracy: 0.8717\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0213 - accuracy: 0.9957 - val_loss: 0.4792 - val_accuracy: 0.8718\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.0137 - accuracy: 0.9970 - val_loss: 0.5036 - val_accuracy: 0.8697\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.5309 - val_accuracy: 0.8682\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import SimpleRNN, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(256, input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjJfTTD3Tl6g",
        "outputId": "d9e6ad52-56dd-41e0-b59c-a65aa318eea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.555500328540802\n",
            "Test accuracy: 0.8751833438873291\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMHZDOECjD3"
      },
      "source": [
        "## 5.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqJJufkff_DF",
        "outputId": "39c7d408-35e6-4bfa-caa4-cefd5640de24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 256)               2311168   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2311682 (8.82 MB)\n",
            "Trainable params: 2311682 (8.82 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 5s 13ms/step - loss: 0.3731 - accuracy: 0.8330 - val_loss: 0.3102 - val_accuracy: 0.8677\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.2673 - accuracy: 0.8871 - val_loss: 0.2999 - val_accuracy: 0.8704\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.2217 - accuracy: 0.9076 - val_loss: 0.3063 - val_accuracy: 0.8708\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1757 - accuracy: 0.9308 - val_loss: 0.3228 - val_accuracy: 0.8696\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1211 - accuracy: 0.9555 - val_loss: 0.3597 - val_accuracy: 0.8714\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.0701 - accuracy: 0.9779 - val_loss: 0.4164 - val_accuracy: 0.8708\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0397 - accuracy: 0.9894 - val_loss: 0.4557 - val_accuracy: 0.8704\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0229 - accuracy: 0.9949 - val_loss: 0.5010 - val_accuracy: 0.8712\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0126 - accuracy: 0.9977 - val_loss: 0.5294 - val_accuracy: 0.8709\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.5708 - val_accuracy: 0.8718\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjgg76RQUFmu",
        "outputId": "7f1af0e7-d679-40ed-fe4a-0d12789e5066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.5813266634941101\n",
            "Test accuracy: 0.8675823211669922\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdDLYruzClXn"
      },
      "source": [
        "## 5.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6HH-Z3R_WqC"
      },
      "outputs": [],
      "source": [
        "# getting data ready\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-77uInOUKhx"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.astype('int')\n",
        "y_val = y_val.astype('int')\n",
        "y_test = y_test.astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67xSe1KqUMDX"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train, 2)\n",
        "y_test = to_categorical(y_test, 2)\n",
        "y_val = to_categorical(y_val, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltaT7YpfUOTm",
        "outputId": "03b9f4f2-562d-46fb-9e19-25a85401419f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train.shape: (35000, 1, 2000), y_train.shape: (35000, 2)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape(-1, 1, X_train.shape[1])\n",
        "X_val = X_val.reshape(-1, 1, X_val.shape[1])\n",
        "X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
        "\n",
        "y_train = y_train.reshape(-1, 2)\n",
        "y_val = y_val.reshape(-1, 2)\n",
        "y_test = y_test.reshape(-1, 2)\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KL4TrU8JURsa",
        "outputId": "9587a4ec-8c29-4386-f57e-a69ff8f1b940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, 256)               1734144   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1734658 (6.62 MB)\n",
            "Trainable params: 1734658 (6.62 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 3s 9ms/step - loss: 0.3707 - accuracy: 0.8465 - val_loss: 0.3013 - val_accuracy: 0.8706\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.2574 - accuracy: 0.8916 - val_loss: 0.2939 - val_accuracy: 0.8789\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.2080 - accuracy: 0.9147 - val_loss: 0.3144 - val_accuracy: 0.8686\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1440 - accuracy: 0.9451 - val_loss: 0.3527 - val_accuracy: 0.8696\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.0839 - accuracy: 0.9713 - val_loss: 0.4184 - val_accuracy: 0.8704\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0414 - accuracy: 0.9875 - val_loss: 0.4910 - val_accuracy: 0.8684\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.0200 - accuracy: 0.9949 - val_loss: 0.5662 - val_accuracy: 0.8713\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.5749 - val_accuracy: 0.8690\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.6253 - val_accuracy: 0.8702\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.6512 - val_accuracy: 0.8706\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "Test loss: 0.6560810208320618\n",
            "Test accuracy: 0.8710494637489319\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(GRU(256, input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())\n",
        "\n",
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExtELYjUT-6",
        "outputId": "1369541f-f0d5-47a7-92a8-10fc25d4173e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "235/235 [==============================] - 1s 2ms/step\n",
            "Label predicted: 1, Actual label: 1\n",
            "text: Although this film is somewhat sanitized (because it was made at a time when people just didn't talk about sex), it is an extremely helpful short film to show prepubescent girls so they know what to expect during menstruation. Not surprisingly, it was paid for by the Kotex company, though what may surprise many is that Disney made this film--as they made a lot of educational films during the 1940s-60s. However well made the film is, though, I think the film maker's missed a real opportunity. Instead of the nice female narrator's voice and the relatively bland visuals it would have been GREAT if they'd used Minnie Mouse and the rest of the Disney gang!! I know this would have given old Walt a heart attack, but wow that would have been a great film! By the way, although the notion of sex is barely hinted at in the film, it DOES adequately explain menstruation in general. However, it does lack some details (especially about intercourse) that I assume were included in the accompanying booklet.<br /><br />Now if only I can figure out why I watched a cartoon about menstruation.\n",
            "Label predicted: 1, Actual label: 1\n",
            "text: The penultimate episode of Star Trek's third season is excellent and a highlight of the much maligned final season. Essentially, Spock, McCoy and Kirk beam down to Sarpeidon to find the planet's population completely missing except for the presence of a giant library and Mr. Atoz, the librarian. All 3 Trek characters soon accidentally walk into a time travel machine into different periods of Sarpeidon's past. Spock gives a convincing performance as an Ice Age Vulcan who falls in love for Zarabeth while Kirk reprises his unhappy experience with time travel--see the 'City on the Edge of Forever'--when he is accused of witchcraft and jailed before escaping and finding the doorway back in time to Sarpeidon's present. In the end, all 3 Trek characters are saved mere minutes before the Beta Niobe star around Sarpeidon goes supernova. The Enterprise warps away just as the star explodes.<br /><br />Ironically, as William Shatner notes in his book \"Star Trek Memories,\" this show was the source of some dispute since Leonard Nimoy noticed that no reason was given in Lisette's script for the reason why Spock was behaving in such an emotional way. Nimoy relayed his misgivings here directly to the show's executive producer, Fred Freiberger, that Vulcans weren't supposed to fall in love. (p.272) However, Freiberger reasoned, the ice age setting allowed Spock to experience emotions since this was a time when Vulcans still had not evolved into their completely logical present state. This was a great example of improvisation on Freiberger's part to save a script which was far above average for this particular episode. While Shatner notes that the decline in script quality for the third season hurt Spock artistically since his character was forced to bray like a donkey in \"Plato's Stepchildren,\" play music with Hippies in \"the Way to Eden\" or sometimes display emotion, the script here was more believable. Spock's acting here was excellent as Freiberger candidly admitted to Shatner. (p.272) The only obvious plot hole is the fact that since both Spock and McCoy travelled thousands of years back in time, McCoy too should have reverted to a more primitive human state, not just Spock. But this is a forgivable error considering the poor quality of many other season 3 shows, the brilliant Spock/McCoy performance and the originality of this script. Who could have imagined that the present inhabitants of Sarpeidon would escape their doomed planet's fate by travelling into their past? This is certainly what we came to expect from the best of 'Classic Trek'--a genuinely inspired story. <br /><br />Shatner, in 'Memories', named some of his best \"unusual and high quality shows\" of season 3 as The Enterprise Incident, Day of the Dove, Is there in Truth no Beauty, The Tholian Web, And the children Shall Lead and The Paradise Syndrome. (p.273) While my personal opinion is that 'And the children Shall Lead' is a very poor episode while 'Is there in Truth no Beauty' is problematic, \"All Our Yesterdays\" certainly belongs on the list of top season three Star Trek TOS films. I give a 9 out of 10 for 'All Our Yesterdays.'\n",
            "Label predicted: 0, Actual label: 0\n",
            "text: Uninspired direction leaves a decent cast stranded in a handsome but bland adaptation, in which dialogue seems recited rather than heartfelt, and cash strapped appearances by the ghosts fail to round up any sense of awe or magic; Edward Woodward, as the Ghost of Christmas Present, wobbles around on stilts and seems to be doing an impression of Bernard Cribbins. As Scrooge, George C. Scott is too wry, and he never seems to truly believe in it, which robs his performance of its effect. The scenes in which he's shown his past have as much impact as if he was half-heartedly flicking through his family album. No one else seems to be putting any effort in, except Frank Finlay, who chronically overacts.\n",
            "Label predicted: 1, Actual label: 1\n",
            "text: I have seen a couple movies on eating disorders but this one was definitely my favorite one. The problem with the other ones was that the people with the eating disorders towards the end just automatically get better or accept the fact that they need help and thats it. this movie I thought was more realistic cause in this one the main character Lexi doesn't automatically just get better. She gets better and then has a drawback. I think this movie shows more than the others that I've seen that getting better doesn't just happen, it's hard work and takes time, it's a long path to recovery. I think this movie shows all of that very well. There should be more movies like this.\n",
            "Label predicted: 0, Actual label: 0\n",
            "text: Let me start out by saying I LOVE horror movies. Big budget, low budget, big name actors, no name actors, it doesn't matter. And when it comes to judging movies I am very forgiving. This movie however, is pretty bad.<br /><br />The actors show little or no emotion when delivering their lines and the acting is worse than many lower budget horror flicks I've seen. As the actors get killed off, you could care less. There is very little gore (I have no idea what film other reviewers watched when they say there is good gore in this one, because there isn't) and the special effects are substandard at best. They steal so much from so many better horror movies (Jeepers Creepers, Friday the 13th, Leprachaun) and it still doesn't help.<br /><br />Luckily I saw this on Showtime and didn't have to actually pay any extra money to see it or waste a spot in my Netflix queue on it. There are so many better horror movies out there and I recommend you see those instead of this big letdown.\n"
          ]
        }
      ],
      "source": [
        "# check predictions\n",
        "from tensorflow.keras.backend import argmax\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "for i in range(5):\n",
        "  print(f'Label predicted: {argmax(y_pred[i]).numpy()}, Actual label: {argmax(y_test[i]).numpy()}')\n",
        "  print(f'text: {test_texts[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sZ0MxIQ_Xs5"
      },
      "source": [
        "## 5.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPFmFxkGU0TC",
        "outputId": "6d6c0167-dac0-45dd-c802-3e2e5697ef76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_1 (Bidirecti  (None, 512)               4622336   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4623362 (17.64 MB)\n",
            "Trainable params: 4623362 (17.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 4s 13ms/step - loss: 0.4118 - accuracy: 0.8213 - val_loss: 0.3001 - val_accuracy: 0.8688\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.2597 - accuracy: 0.8931 - val_loss: 0.2920 - val_accuracy: 0.8779\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.2131 - accuracy: 0.9133 - val_loss: 0.3194 - val_accuracy: 0.8706\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1582 - accuracy: 0.9385 - val_loss: 0.3295 - val_accuracy: 0.8730\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1006 - accuracy: 0.9650 - val_loss: 0.3639 - val_accuracy: 0.8736\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.0573 - accuracy: 0.9839 - val_loss: 0.4244 - val_accuracy: 0.8682\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.0311 - accuracy: 0.9924 - val_loss: 0.4590 - val_accuracy: 0.8710\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.0170 - accuracy: 0.9964 - val_loss: 0.5088 - val_accuracy: 0.8685\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.5387 - val_accuracy: 0.8690\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 0.5740 - val_accuracy: 0.8705\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(256), input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tADCJRBeVb3m",
        "outputId": "45be870a-a59a-42fc-a79f-1e3be211631b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.5876869559288025\n",
            "Test accuracy: 0.8723829984664917\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38LE-QeKGYad"
      },
      "source": [
        "## 5.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHyV8Myo_Xs6"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "RNN\n",
        " - Test Loss: 0.555500328540802\n",
        " -Test Accuracy: 0.8751833438873291\n",
        "\n",
        "LSTM\n",
        "   - Test Loss: 0.5813266634941101\n",
        "   - Test Accuracy: 0.8675823211669922\n",
        "\n",
        "GRU\n",
        "   - Test Loss: 0.6560810208320618\n",
        "   - Test Accuracy: 0.8710494637489319\n",
        "\n",
        "BiLSTM\n",
        "   - Test Loss: 0.5876869559288025\n",
        "   - Test Accuracy: 0.8723829984664917\n",
        "\n",
        "\n",
        "The RNN model has the lowest test loss and highest accuracy of the four models at  0.555500 and 0.8751833 respectively. This is a bit unusual.\n",
        "The RNN's performance indicates that this dataset with these conditions the more simple model accurately the important information for sentiment analysis and more complex models did provide an advantage.\n",
        "\n",
        "Here the LSTM model had about 0.03 more test loss and about 0.1 less test accuracy. The GRU model had 0.1 more test loss and about 0.004 less accuracy. The BiLSTM model had about 0.03 more test loss and a 0.003 less accuracy.\n",
        "\n",
        "LSTM and GRU models are meant to prevent the vanishing gradient probelem and the RNN and BiLSTM models are meant to find the patterns that go both directions of the sequence. Potentially the learning rate may not have been opitmal for the three other models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "my_env",
      "language": "python",
      "name": "my_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}