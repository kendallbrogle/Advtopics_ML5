{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To copy this template: File -> Save a Copy in Drive\n",
        "\n",
        "***DISCLAIMER**: In case of any discrepancy in the assignment instruction, please refer to the `PDF` document.*"
      ],
      "metadata": {
        "id": "OMAdi9qgC-B9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3 - Attention in Transformer"
      ],
      "metadata": {
        "id": "NcDhlfqyBd6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1"
      ],
      "metadata": {
        "id": "-id00ye6CNLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "There is a total of 3 vectors derived from the self attention block in transformers. They are called Query, Key and, Value vectors. Together they determine how much attention that input should pay to other elements in the sequence.\n",
        "The query vector is utilized to determine the relevance (query) of the other vectors. Iti si lookign for which key is most relevant.\n",
        "The key vector is a representation of the input vector compared the the query.\n",
        "The value vector is what contains the information of the input vector used in the computation.\n",
        "Based on the query and key vecotrs the the value vector is what is used. Each input vector has its own three query key and value vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "4kineJbrfcg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2"
      ],
      "metadata": {
        "id": "e1D_yfneCWqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax function is calculated by:\n",
        "\n",
        "Attention(Q, K, V) = softmax(((Q * K^T) / sqrt(d_k)) * V)"
      ],
      "metadata": {
        "id": "HlXb30L1poTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "To calculate this we first take the dot product of Q and K. This gives the attention score of each pair of K adn Q to measure their compatibility. We then scale the dot products by taking the sq root of the dimensionality of the vectors. This precvents small gradients.\n",
        "The softmax function is then applied to each set of scores which makes the scaled dot products equal to probabilities. These probabilites help the model pick which words should be doucsed on.\n",
        "The probabilites from the soft max fucntino are the weights of the value vectors. The weighted value vectors are then added to create the final output."
      ],
      "metadata": {
        "id": "BX8Ns_Imf-BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3"
      ],
      "metadata": {
        "id": "TpMHZDOECjD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-headed attention gives the attention layer multiple ”representation subspaces”. If we have 8\n",
        "heads in a self-attention block each with input vectors of size 512 and output vectors of size 512, how\n",
        "many weight matrices do we need to learn in total across these 8 heads? What is the size of these\n",
        "matrices for an input of size 4 word embeddings?"
      ],
      "metadata": {
        "id": "SXd10qmSumh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "If each of 8 heads has an input of 512 size vectors and the same size output then each query key and value matrix would be 512 x 512.\n",
        "If theres 8 heads, each needs 3 matrices making it a total of 24 weight matrices. If there is an output matrix then there would be 8 more making it 32 weight matrices.\n",
        "\n",
        "The size of the input does not change the size of the weighted output matrices, so the query, key, and vector matrices wouls still be 512 x 512 even if there was an inoyt of size 4 word embeddings."
      ],
      "metadata": {
        "id": "GRo9fm0tf-ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4"
      ],
      "metadata": {
        "id": "Ri59wswo8wSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feed-forward layer following the self-attention is expecting a single matrix (a vector for each word).\n",
        "How can we go from the output of multiple heads to a single matrix input for the feed-forward layer?"
      ],
      "metadata": {
        "id": "oR3T1gHaxNuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "To obtain a singel matrix input for the feed forward layer we ned to combine the output of the heads to pass through the feed forward neural netowwork, which is done through concatinating and linear projection. Specifically, each head mutli headed attention creates an output matrix with a single vector for each word in the input sequence. The vectors are then concatinated along the dimension axis to create one long vector for each word. This is the concatination process. Then linear projection follows. The concated vector is too long for what the feed forward layer so it needs to be reduces. This is done through linear projection of weight Wo. The matrix is of size dmodel x (h x dk) where dmodel is the size of the input word imbedding, h is the number of heads and dk is the size of the output vector from each head. After linear projection there is a singel matrix with one vector representing one word of the original embedding size. This is the matrix used in the feed forward neural network layer."
      ],
      "metadata": {
        "id": "FF8uLB3J8wSp"
      }
    }
  ]
}